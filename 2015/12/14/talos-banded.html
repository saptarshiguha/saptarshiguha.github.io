<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>Detecting Banded Figures in Talos Suites</title>
   <meta name="author" content="Saptarshi Guha" />
  <meta name="viewport" content="width=320, user-scalable=yes initial-scale=0.7">
   <script type="text/javascript" src="https://code.jquery.com/jquery-latest.min.js"></script>
   <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
         crossorigin="anonymous">
   <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css"
         crossorigin="anonymous">
   <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" crossorigin="anonymous"> </script>
   <link rel="stylesheet" href="https://saptarshiguha.github.io//css/syntax.css" type="text/css" />
   <!-- Homepage CSS -->
   <link rel="stylesheet" href="https://saptarshiguha.github.io//css/journal.css" type="text/css" media="screen, projection"/>
   <link href="https://fonts.googleapis.com/css?family=PT+Serif+Caption" rel="stylesheet">
   <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet">
   <link rel="stylesheet" href="https://saptarshiguha.github.io//js/fancybox/jquery.fancybox.css" type="text/css" media="screen" />

   <!-- mathjax config similar to math.stackexchange -->
   <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      messageStyle: "none",
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
    });
   </script>

   
   <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
   <script src="https://saptarshiguha.github.io//js/fleximage/jquery.flex-images.js"></script>
   <script type="text/javascript" src="https://saptarshiguha.github.io//js/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
   <link href='https://people.mozilla.com/~sguha/blog/feed.xml'  rel='alternate' type='application/atom+xml'>

</head>
<body>
<div class="container-fluid">

  <div class="row" style="margin-top:1em;">
    <div class="col-xs-6 col-xs-offset-3 row-centered">
      <div class="col-md-4 col-md-offset-4  col-xs-8 col-xs-offset-2 row-centered">
        <div class="maintitle">
          <figure class="figure">
            <a href="https://saptarshiguha.github.io/"> <img src="https://saptarshiguha.github.io//css/log1.png" class="figure-img img-fluid img-rounded" alt="aajikali"> </a>
          </figure>
        </div> 
      </div>
    </div>
  </div>
  
    <!-- The content is contained inside it's own div which inside body -->
  <div id="post">

<div class="row row-centered "><div class="col-lg-4 col-md-6 col-xs-10  col-sm-10 col-centered">
<div class="jrnl">
        
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#clustering" id="markdown-toc-clustering">Clustering</a></li>
  <li><a href="#mixture-of-gaussians" id="markdown-toc-mixture-of-gaussians">Mixture of Gaussians</a></li>
  <li><a href="#comparison" id="markdown-toc-comparison">Comparison</a></li>
  <li><a href="#but-when-to-use" id="markdown-toc-but-when-to-use">But When to Use?</a></li>
  <li><a href="#other-methods" id="markdown-toc-other-methods">Other Methods</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>The objective here is to label figures such as the one below (call the shape of
the graph as ‘A’) as a banded figure.Now terminology can be varied. One can say
that the density of the observations is clearly bimodal. And indeed it is.</p>

<table class="image" align="center">
<caption align="bottom"></caption>
<tr><td align="center"> <img src="https://saptarshiguha.github.io//images/talos/t1.png" width="55%" alt="" /></td></tr>
</table>

<p>But consider this figure (B),which our algorithm will
observe and need to classify</p>

<table class="image" align="center">
<caption align="bottom"></caption>
<tr><td align="center"> <img src="https://saptarshiguha.github.io//images/talos/t2.png" width="55%" alt="" /></td></tr>
</table>

<p>This is case of a level shift. In each regime (or level) the density of the
observations are unimodal but combined the density is definitely bimodal. The
rub here is that the observations are not identically distribution and most
schemes for checking bimodality (or mixtures) assume the data is identically
distributed.</p>

<p>So we have a classification problem and need a way to assign figures such as A
above to the banded class. We have two approaches. Each have aspects to them
that are more intuitive than others.</p>

<h2 id="clustering">Clustering</h2>

<p>On the y-axis the observations in A clearly belong to two clusters. The idea is
that if two <em>distinct</em> clusters do exist, then a k-means clustering algorithm
repeated many times (since the k-means is sensitive to seed centers) will be
able to detect such clusters. Indeed it does (with our data). Let’s assume we
can assign each observation to 2 detected clusters (these may not be ‘real’
clusters). Some observations follow:</p>

<ol>
  <li>
    <p>The classifier is sensitive to the proportion of the two clusters. If one is
much larger than the other (typical sample size is 60-100 observations) say
in the ratio of 85:15, then the figure looks like a case of one cluster and
several outliers. See this figure</p>

    <table class="image" align="center">
 <tr><td align="center"> <img src="https://saptarshiguha.github.io///images/talos/t4.png" width="65%" /></td></tr>
 </table>

    <p>What we are trying to detect is a regular switching between two generating
distributions.  So this ratio becomes a parameter. We want to be able detect
figures with two ‘strongly developed’ bands.</p>
  </li>
  <li>
    <p>As we’ll see in point (4), we have a cutoff based on distance. To make this
cutoff make sense across all figures which can have different scales, we
standardize the data to unit scale and zero mean. Also we cap outliers to say
+/- 4. Though this is a parameter, the rule is not very much affected by it,
mostly because we don’t have too many outliers.</p>
  </li>
  <li>
    <p>As mentioned above, k-means can return different clusters depending on the
seed. If there are two distinct clusters it is expected that in N
repetitions, these clusters will appear majority number of times. This N is
also  a parameter.</p>
  </li>
  <li>
    <p>The k-means algorithm can return false positives. For example, the k-means
will return two clusters when given data from $Uniform(0,1)$. To identify
sharply defined clusters we create a ‘distance’ measure. There are two
clusters with centers $C_1$ and $C_2$. For a given observation $Y_i$ let it
be assigned to cluster $C_i$. Then $\frac{ | Y_i - C_i|}{ |Y_i-C_1| + |
Y_i-C_2|}$ is a measure of how strongly the $Y_i$ belongs to cluster $C_i$. The
mean of this across all points $Y_i$ is a measure of how strong the
clustering is. This mean ought to be small.</p>

    <p>We also confirm that means of the observations in each cluster
are statistically different. <em>I’m not sure if this second condition is   actually necessary</em></p>
  </li>
  <li>
    <p>We also do not want the figures such as B to tagged as banded. There are two approaches.</p>

    <p>a) Assign the points indices 1 to number of observations. If it is a level
shift then the group of indices in one cluster will have a different center
than the indices in the second group. A Wilcoxon test will suffice to check
this. A problem with this, as in the below figure is that it will reject an
obviously bandied graph that has shown a level shift.</p>

    <table class="image" align="center">
 <tr><td align="center"> <img src="https://saptarshiguha.github.io///images/talos/t3.png" width="55%" /></td></tr>
 </table>

    <p>b) A way around this is to divide the x-axis into windows of size $h$ and
compute the above the algorithm for each window. If the measure in (4) are
acceptable in all the windows, then we can conclude the curve is banded even
if it has a level shift. This approach will reject figures such as B but
accept the above figure since it is banded in each window. This approach is
sensitive to the number of observations in the window and works if we have
plenty of data. Which is not <em>always</em> the case.</p>
  </li>
  <li>
    <p>Ultimately, we have a rule, once we have clusters, if</p>
    <ul>
      <li>the means of the cluster sufficiently different?</li>
      <li>there is no level shift (5a is not rejected)</li>
      <li>the separation score (defined in (4)) is less than some cut off (in our case 0.14)</li>
    </ul>

    <p>then we consider the figure as banded.</p>
  </li>
</ol>

<p>See <a href="http://people.mozilla.org/~sguha/tmp/bandiness.html">this link</a> (be
patient, 3MB of 220 bokeh plots)  for an example of
figures marked as banded (each plot has a number id:## x1_x2 below it.
Figures with x2==0 are labelled as banded). The code for this is
<a href="https://gist.github.com/anonymous/b182c1a175171117579b">https://gist.github.com/anonymous/b182c1a175171117579b</a>.
There are 220 combinations of platform, test suite and compile options.</p>

<h2 id="mixture-of-gaussians">Mixture of Gaussians</h2>

<p>This is a statistical model based approach. Here we consider the observations to
be independent and identically distributed (as we’ll see we’ll handle the not
identical bit in a manner similar to 5(a) above) from</p>

<script type="math/tex; mode=display">Y_i \sim \lambda_1Normal(\mu_1,\sigma_1)+ (1-\lambda)Normal(\mu_2, \sigma_2)</script>

<p>where $0 &lt; \lambda &lt; 1$. Using the
<a href="https://cran.r-project.org/web/packages/mixtools/index.html">mixtools</a> package
in R, the above model is fit using an iterative Expectation Maximization(EM)
approach. What it returns us (after several iterations)</p>

<ul>
  <li>the value of $\lambda$</li>
  <li>the values of $\mu$ and $\sigma$ for each component</li>
  <li>the posterior probabilities of $Y_i$ belonging to each component</li>
</ul>

<p>We classify an observation $Y_i$ belonging to cluster $C_i$ if the posterior
probability of it belong to cluster $C_i$ is greatest. The classification rule
for this is a bit similar to the the clustering approach.</p>

<ol>
  <li>We check that $\lambda$ is not too close to 0 or 1, for then we have a case
similar to (1) in the clustering approach.</li>
  <li>In a given cluster (clusters 1 or 2), for every point we compute the
difference in posterior probability of belonging to component 1 or 2. This is
averaged across the cluster (for each cluster) and we take the minimum
value. We apply a cutoff 0.9.  That is in both clusters the average
probability of belonging to the assigned cluster is more than probability of
belonging to the other cluster by at least 0.9</li>
  <li>We apply a similar condition as 5(a) to reject level shifts</li>
</ol>

<p>A link to these figures can be found(be patient, 3MB of 220 bokeh plots)
<a href="http://people.mozilla.org/~sguha/tmp/bandi/alltypes.html">here</a>. Code for this
is <a href="https://gist.github.com/anonymous/c5a0fcf3785ddc589cd7">here</a>.</p>

<p>A natural question, isn’t there a hypothesis test for whether one component is
better than two (or vice versa)? This is a complicated problem and often
involves visual inspection. Using this test and then applying rules (like above)
wouldn’t make it much more sophisticated. However this is something I will look
into more.</p>

<h2 id="comparison">Comparison</h2>
<p>Using the rules above they both work very well. And indeed</p>

<ul>
  <li><a href="http://people.mozilla.org/~sguha/tmp/bandi/kmPos_mixNeg.html">this page</a> is the list of all platform/test suite/compile options that were
flagged by kmeans but <em>not</em> by the mixture approach</li>
  <li><a href="http://people.mozilla.org/~sguha/tmp/bandi/kmNeg_mixPos.html">this page</a> are
the figures considered by banded by the mixture approach but <em>not</em> by the
kmeans approach.</li>
</ul>

<p>The</p>

<ul>
  <li>kmeans approach is fairly easy to understand and implement.</li>
  <li>the mixture model is probabilistic and the rule in (2) for the mixture model is
easier to understand that the one in (4) for kmeans.</li>
  <li>we do not need to standardize the observations in the mixture method since we
discriminate on the probability scale. This can be fixed in kmeans by using
probabilistic clustering.</li>
  <li>both exist in Python.</li>
</ul>

<p>Given this, <em>I would recommend  the kmeans approach</em> since it is very easy to
understand even for someone not familiar with  probability. Moreover, we can expand the
kmeans approach to a probabilistic clustering approach to match something
similar to the mixture component method.</p>

<h2 id="but-when-to-use">But When to Use?</h2>

<p>So when is this important? We can always not worry about the bimodality and
ignore this information. If we use a t-test to compare means then we increase
the standard error of the test (because the bimodal distribution will have a
larger standard deviation). Thus when comparing new data with old using a t-test
and ignoring bimodality, we tend to <em>not reject</em> the null hypothesis of no
change.</p>

<p>But if we are trying to detect very large changes for a group of new
observations , then we not so affected. But this all depends on the nature of
the standard deviation with respect to the means of the reference data.</p>

<p>In general, if we can conclude the data is bimodal, we classify observations to
each component/cluster. We could construct a test with improved efficiency with
this new information.</p>

<h2 id="other-methods">Other Methods</h2>

<p>The first method that comes to mind is Kernel Density estimation (KDE). In this
approach, the most sensitive parameter is the <em>bandwidth</em> $h$. KDE methods can also
be non parametric which removes the Gaussian assumption. Some guiding parameters
would still need to be chosen such as the mixing proportion ($\lambda$ in the
Mixture section)</p>


</div>          
</div></div>


 <br>
</div>



<div class="row text-center " style="margin-bottom:0.5em;">
  <div id="direction">
    
      <span class="previous-link" style='font-size:75%;'>
         <a style='text-decoration: none;border-bottom:none;' rel="prev" href="https://saptarshiguha.github.io///2015/12/02/talos-canvasmark.html">Older</a>
      </span>
    
    
       <span class="next-link"  style='font-size:75%;'>
         <a  style='text-decoration:none;border-bottom:none;'  rel="next" href="https://saptarshiguha.github.io///2015/12/20/rain.html">&nbsp&nbspNewer</a>
       </span>
    
  </div>
</div>



<script>
 $(document).ready(function() {
   $("a[href*='drive.google.com/'],a[href*='docs.google.com/'],a[href*='.png'],a[href*='.jpg']").attr('rel', 'gallery').fancybox({
     padding:5,margin:5,aspectRatio:true,
     prevEffect	: 'none',
     nextEffect	: 'none',
     type: "image"
   });
     $('tr').parent('thead').parent('table').addClass('table table-condensed');
   console.log("FOOOO");
 })
</script>




</div>
</body>
</html>
